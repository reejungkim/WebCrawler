{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "검색어='구글증기'\n",
    "raw = requests.get(\"https://search.naver.com/search.naver?where=news&query=\"+검색어)\n",
    "html = BeautifulSoup(raw.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 약간의 제안 사항이 있다.\n",
    "\n",
    "1) API 호출이 일 25,000회로 제한되어 있다. 최대 한 번 호출에 100개의 검색을 가져 올 수 있다.\n",
    "\n",
    "2) 네이버에서 데이터를 검색하면 뉴스에서만 약 43만건의 데이터가 검색된다. 하지만 최대 1000개만 API를 이용하여 가지고 올 수 있다.\n",
    "\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time   \n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/reejungkim/Documents/Git/OpenAPI_Ministry_of_land/.env')\n",
    "\n",
    "url = 'https://openapi.naver.com/v1/search/news?'\n",
    "clientId = os.environ.get( 'Naver_News_clientId')\n",
    "clientSecret = os.environ.get('Naver_News_clientSecret')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 크롤러 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-14 00:52:21.106493] Url Request Success\n",
      "[2023-01-14 00:52:21.307530] Url Request Success\n",
      "[2023-01-14 00:52:21.511205] Url Request Success\n",
      "[2023-01-14 00:52:21.703768] Url Request Success\n",
      "[2023-01-14 00:52:21.926196] Url Request Success\n",
      "[2023-01-14 00:52:22.130035] Url Request Success\n",
      "[2023-01-14 00:52:22.333159] Url Request Success\n",
      "[2023-01-14 00:52:22.571172] Url Request Success\n",
      "[2023-01-14 00:52:22.799149] Url Request Success\n",
      "[2023-01-14 00:52:23.021353] Url Request Success\n",
      "HTTP Error 400: Bad Request\n",
      "[2023-01-14 00:52:23.091246] Error for URL : https://openapi.naver.com/v1/search/news.json?query=%EA%B5%AC%EA%B8%80&start=1001&display=100\n",
      "구글_naver_news.json SAVED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "#from config import *\n",
    "\n",
    "#[CODE 1]\n",
    "def get_request_url(url):\n",
    "    \n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"X-Naver-Client-Id\", clientId)\n",
    "    req.add_header(\"X-Naver-Client-Secret\", clientSecret)\n",
    "    try: \n",
    "        response = urllib.request.urlopen(req)\n",
    "        if response.getcode() == 200:\n",
    "            print (\"[%s] Url Request Success\" % datetime.datetime.now())\n",
    "            return response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"[%s] Error for URL : %s\" % (datetime.datetime.now(), url))\n",
    "        return None\n",
    "\n",
    "#[CODE 2]\n",
    "def getNaverSearchResult(sNode, search_text, page_start, display):\n",
    "    \n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = \"/%s.json\" % sNode\n",
    "    parameters = \"?query=%s&start=%s&display=%s\" % (urllib.parse.quote(search_text), page_start, display)\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    retData = get_request_url(url)\n",
    "    \n",
    "    if (retData == None):\n",
    "        return None\n",
    "    else:\n",
    "        return json.loads(retData)\n",
    "\n",
    "#[CODE 3]\n",
    "def getPostData(post, jsonResult):\n",
    "    \n",
    "    title = post['title']\n",
    "    description = post['description']\n",
    "    org_link = post['originallink']\n",
    "    link = post['link']\n",
    "\n",
    "    #Tue, 14 Feb 2017 18:46:00 +0900\n",
    "    pDate = datetime.datetime.strptime(post['pubDate'],  '%a, %d %b %Y %H:%M:%S +0900')\n",
    "    pDate = pDate.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    jsonResult.append({'title':title, 'description': description,\n",
    "                    'org_link':org_link, 'link': org_link, \n",
    "                    'pDate':pDate})\n",
    "    return    \n",
    "\n",
    "def main():\n",
    "\n",
    "    jsonResult = []\n",
    "\n",
    "    # 'news', 'blog', 'cafearticle'\n",
    "    sNode = 'news'\n",
    "    search_text = '구글'\n",
    "    display_count = 100\n",
    "    \n",
    "    jsonSearch = getNaverSearchResult(sNode, search_text, 1, display_count)\n",
    "    \n",
    "    while ((jsonSearch != None) and (jsonSearch['display'] != 0)):\n",
    "        for post in jsonSearch['items']:\n",
    "            getPostData(post, jsonResult)\n",
    "        \n",
    "        nStart = jsonSearch['start'] + jsonSearch['display']\n",
    "        jsonSearch = getNaverSearchResult(sNode, search_text, nStart, display_count)\n",
    "    \n",
    "    \n",
    "    with open('%s_naver_%s.json' % (search_text, sNode), 'w', encoding='utf8') as outfile:\n",
    "        retJson = json.dumps(jsonResult,\n",
    "                        indent=4, sort_keys=True,\n",
    "                        ensure_ascii=False)\n",
    "        outfile.write(retJson)\n",
    "        \n",
    "    print ('%s_naver_%s.json SAVED' % (search_text, sNode))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>org_link</th>\n",
       "      <th>pDate</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>필자가 일하는 &lt;b&gt;구글&lt;/b&gt;의 사명은 ‘세상의 모든 정보를 잘 정리해서 모든 사...</td>\n",
       "      <td>https://www.hankyung.com/opinion/article/20230...</td>\n",
       "      <td>https://www.hankyung.com/opinion/article/20230...</td>\n",
       "      <td>2023-01-14 00:25:00</td>\n",
       "      <td>[한경에세이] 나는 왜 일하는가?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서대문구 세검정로 134 유원하나아파트 앞 상수도관 파열(구경300㎜) (출처=&lt;b...</td>\n",
       "      <td>https://www.gukjenews.com/news/articleView.htm...</td>\n",
       "      <td>https://www.gukjenews.com/news/articleView.htm...</td>\n",
       "      <td>2023-01-14 00:06:00</td>\n",
       "      <td>서울 홍제동 &amp;quot;수도물 단수시간&amp;quot; 상수도관 누수파열</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14일 영국 가격 비교 웹사이트 컴페어 더 마켓(Compare the market)...</td>\n",
       "      <td>https://www.theguru.co.kr/news/article.html?no...</td>\n",
       "      <td>https://www.theguru.co.kr/news/article.html?no...</td>\n",
       "      <td>2023-01-14 00:02:00</td>\n",
       "      <td>전 세계에서 가장 많이 검색된 자동차 브랜드?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이와 관련해 유튜브를 운영하는 &lt;b&gt;구글&lt;/b&gt;의 언론 담당자는 해당 계정이 &lt;b&gt;...</td>\n",
       "      <td>https://www.chosun.com/politics/north_korea/20...</td>\n",
       "      <td>https://www.chosun.com/politics/north_korea/20...</td>\n",
       "      <td>2023-01-13 23:51:00</td>\n",
       "      <td>평양 ‘브이로그’ 유튜버 또 등장... 유창한 영어 쓰는 ‘유미’의 정체는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>게임은 물론, 소셜미디어와 비디오 스트리밍 앱은 해당 기간 &lt;b&gt;구글&lt;/b&gt;플레이와...</td>\n",
       "      <td>https://www.donga.com/news/article/all/2023011...</td>\n",
       "      <td>https://www.donga.com/news/article/all/2023011...</td>\n",
       "      <td>2023-01-13 22:35:00</td>\n",
       "      <td>[기고] 2023년 &amp;apos;다섯번째&amp;apos; 분기의 마케팅 극대화 방법은?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  필자가 일하는 <b>구글</b>의 사명은 ‘세상의 모든 정보를 잘 정리해서 모든 사...   \n",
       "1  서대문구 세검정로 134 유원하나아파트 앞 상수도관 파열(구경300㎜) (출처=<b...   \n",
       "2  14일 영국 가격 비교 웹사이트 컴페어 더 마켓(Compare the market)...   \n",
       "3  이와 관련해 유튜브를 운영하는 <b>구글</b>의 언론 담당자는 해당 계정이 <b>...   \n",
       "4  게임은 물론, 소셜미디어와 비디오 스트리밍 앱은 해당 기간 <b>구글</b>플레이와...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.hankyung.com/opinion/article/20230...   \n",
       "1  https://www.gukjenews.com/news/articleView.htm...   \n",
       "2  https://www.theguru.co.kr/news/article.html?no...   \n",
       "3  https://www.chosun.com/politics/north_korea/20...   \n",
       "4  https://www.donga.com/news/article/all/2023011...   \n",
       "\n",
       "                                            org_link                pDate  \\\n",
       "0  https://www.hankyung.com/opinion/article/20230...  2023-01-14 00:25:00   \n",
       "1  https://www.gukjenews.com/news/articleView.htm...  2023-01-14 00:06:00   \n",
       "2  https://www.theguru.co.kr/news/article.html?no...  2023-01-14 00:02:00   \n",
       "3  https://www.chosun.com/politics/north_korea/20...  2023-01-13 23:51:00   \n",
       "4  https://www.donga.com/news/article/all/2023011...  2023-01-13 22:35:00   \n",
       "\n",
       "                                          title  \n",
       "0                            [한경에세이] 나는 왜 일하는가?  \n",
       "1         서울 홍제동 &quot;수도물 단수시간&quot; 상수도관 누수파열  \n",
       "2                     전 세계에서 가장 많이 검색된 자동차 브랜드?  \n",
       "3     평양 ‘브이로그’ 유튜버 또 등장... 유창한 영어 쓰는 ‘유미’의 정체는  \n",
       "4  [기고] 2023년 &apos;다섯번째&apos; 분기의 마케팅 극대화 방법은?  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(r'/Users/reejungkim/Documents/Git/WebCrawler/구글_naver_news.json')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e47b1a34c05c1e3b83a62d7885c9d1b5ef8a0522d3be0182d0a008ec409b2b3d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
